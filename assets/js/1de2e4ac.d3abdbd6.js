"use strict";(self.webpackChunkuc_2_docs=self.webpackChunkuc_2_docs||[]).push([[3308],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>u});var o=t(67294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function r(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=o.createContext({}),p=function(e){var n=o.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},c=function(e){var n=p(e.components);return o.createElement(l.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},m=o.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),m=p(t),u=a,h=m["".concat(l,".").concat(u)]||m[u]||d[u]||i;return t?o.createElement(h,s(s({ref:n},c),{},{components:t})):o.createElement(h,s({ref:n},c))}));function u(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,s=new Array(i);s[0]=m;var r={};for(var l in n)hasOwnProperty.call(n,l)&&(r[l]=n[l]);r.originalType=e,r.mdxType="string"==typeof e?e:a,s[1]=r;for(var p=2;p<i;p++)s[p]=t[p];return o.createElement.apply(null,s)}return o.createElement.apply(null,t)}m.displayName="MDXCreateElement"},13204:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>p});var o=t(87462),a=(t(67294),t(3905));const i={},s="openUC2 Microscopy Workshop @ AQLM/MBL Woods Hole",r={unversionedId:"Toolboxes/DiscoveryElectronics/DigitalMicroscopy",id:"Toolboxes/DiscoveryElectronics/DigitalMicroscopy",title:"openUC2 Microscopy Workshop @ AQLM/MBL Woods Hole",description:"Welcome to the openUC2 Workshop on Modular and Digital Microscopy. This hands-on session will introduce participants\u2014especially those with a biology background\u2014to accessible, customizable microscopy using open-source tools. We will walk through building and extending microscopes, from simple lens assemblies to digital microscopy. \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udd2c",source:"@site/docs/01_Toolboxes/02_DiscoveryElectronics/06_DigitalMicroscopy.md",sourceDirName:"01_Toolboxes/02_DiscoveryElectronics",slug:"/Toolboxes/DiscoveryElectronics/DigitalMicroscopy",permalink:"/docs/Toolboxes/DiscoveryElectronics/DigitalMicroscopy",draft:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"openUC2 *Spectrometer*",permalink:"/docs/Toolboxes/DiscoveryElectronics/spectrometer"},next:{title:"openUC2 XIAO Microscope Documentation",permalink:"/docs/Toolboxes/DiscoveryElectronics/04_1_seeedmicroscope"}},l={},p=[{value:"Installation of Jupyter Notebook",id:"installation-of-jupyter-notebook",level:2},{value:"Getting to know the camera",id:"getting-to-know-the-camera",level:3},{value:"Acquiring the images at varying illumination and merge them",id:"acquiring-the-images-at-varying-illumination-and-merge-them",level:3},{value:"Compute the DPC images",id:"compute-the-dpc-images",level:3},{value:"Introduce the qDPC Model",id:"introduce-the-qdpc-model",level:3},{value:"Chapter 6: Light-Sheet Microscopy Hands-On (Optional)",id:"chapter-6-light-sheet-microscopy-hands-on-optional",level:2},{value:"Goals",id:"goals",level:3},{value:"Resources",id:"resources",level:3},{value:"Instructions",id:"instructions",level:3},{value:"Summary &amp; Discussion",id:"summary--discussion",level:2}],c={toc:p};function d(e){let{components:n,...i}=e;return(0,a.kt)("wrapper",(0,o.Z)({},c,i,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"openuc2-microscopy-workshop--aqlmmbl-woods-hole"},"openUC2 Microscopy Workshop @ AQLM/MBL Woods Hole"),(0,a.kt)("p",null,"Welcome to the openUC2 Workshop on Modular and Digital Microscopy. This hands-on session will introduce participants\u2014especially those with a biology background\u2014to accessible, customizable microscopy using open-source tools. We will walk through building and extending microscopes, from simple lens assemblies to digital microscopy. \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udd2c"),(0,a.kt)("p",null,":::warn"),(0,a.kt)("h2",{id:"installation-of-jupyter-notebook"},"Installation of Jupyter Notebook"),(0,a.kt)("p",null,"We assume you have a python instance (e.g. inside a anaconda/mamba environment) available.\nThen you can do:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'pip install jupyterlab\n``\n\n\n## Scientific Background\n\nMicroscopy is important -obviously! Traditional microscopes, however, can be expensive, inflexible, and non-transparent in design.\nThe openUC2 ecosystem offers an open-source, modular alternative. With 3D-printed components, low-cost electronics, and standardized 50mm cube modules, users can build anything from a simple smartphone microscope to a fully automated fluorescence light-sheet setup.\n\nFeel free to share any of your data under **`#openUC2AQLM`\n\n*What you are going to learn:*\n\n\n| Chapter | Topic                              | Est. Time |\n| ------- | ---------------------------------- | --------- |\n| 1       | openUC2 **coreBOX** basics         | 45\xa0min    |\n| 2       | openUC2 **Seeed** Microscope setup | 30\xa0min    |\n| 3       | Fluorescence add\u2011on                | 30\xa0min    |\n| 4       | LED\u2011Matrix illumination & contrast | 45\xa0min    |\n| 5       | Jupyter workflow\xa0(DPC)             | 60\xa0min    |\n| 6       | Light\u2011sheet hands\u2011on               | 60\xa0min    |\n\n\n## Some References\n\nPlease go through the following links to check about the:\n- [coreBOX](https://openuc2.github.io/docs/Toolboxes/DiscoveryCore/ENGLISH/uc2miniboxEN)\n-  [openUC2 x Seeed Studio AI Microscope](https://openuc2.github.io/docs/Toolboxes/DiscoveryElectronics/04_1_seeedmicroscope)\n- [openUC2 Electronics](https://openuc2.github.io/docs/Toolboxes/DiscoveryElectronics/automation_intro)\n\n\n## Chapter 1: Getting Started with the openUC2 coreBOX\n\n### Goals\n\n* Unbox & inspect components\n* Understand the concept of modular optics\n* Build a **Kepler telescope**\n* Convert setup into a **smartphone microscope**\n\n### Instructions\n\n0. **Read the manual** (QR\u2011code inside lid) \u2013 note safety icons.\n1. Unpack your **coreBOX** and review the included printed or digital manual.\n2. Assemble a **[simple telescope](https://openuc2.github.io/docs/Toolboxes/DiscoveryCore/ENGLISH/CoreTelescope)** to understand lens focusing.\n   a. Assemble two lens holders at 50\xa0mm spacing.\n   b. Insert eyepiece (10\xd7) & objective (50\xa0mm\xa0f).\n   c. Align by sliding cubes until full\u2011field image is sharp.\n3. Build the **[smartphone microscope](https://openuc2.github.io/docs/Toolboxes/DiscoveryCore/ENGLISH/coreMicroscope#smartphone-microscope)** using the RMS objective holder and phone adapter.\n   a. Adjust the setup depending on your phone (add a cube spacer in between eventually)\n   b. Mount sample slide on slide\u2011holder cube.\n   c. Use flashlight cube for epi\u2011illumination.\n4. Place a sample and capture an image.\n\nFind a cool sample and take pictures of your setup and share them on social media using `#openUC2AQLM` if you like :)\n\n\n## Chapter 2: Upgrading Illumination with LED Matrix\n\n### Goals\n\n* Replace the flashlight with programmable LED matrix\n* Explore the impact of illumination patterns on image contrast\n\n### Theory\n\nIllumination geometry affects how (especially visible with transparent) samples scatter light. Oblique, ring-shaped, or side-specific lighting can enhance features invisible under uniform illumination.\n\n### Instructions\n\n1. Replace the flashlight with the **openUC2 LED matrix**.\n2. Flash the firmware onto the LED controller (see LED Matrix Flash Guide).\n3. Connect to the board using the webserial webpage and test control commands: `left`, `right`, `top`, `bottom`, `ring`, etc.\n4. View your sample again using the matrix lighting and observe contrast changes:\n\n   * Outer ring: **Darkfield illumination** (black background, white features)\n   * Side lighting: **Gradient effects**\n\nTake photos of the visual effects and share your results and post it if you like :)\n\n\n#### Flash the firmware to the openUC2 LED Matrix\n\n\n*Go to this website: youseetoo.github.io and flash the firmware (**WAVEHSARE MATRIX**)*\n![](./IMAGES/wavesharematrix/waveshareflash_1.png)\n\n*Select the port*\n![](./IMAGES/wavesharematrix/waveshareflash_2.png)\n\n*Select INstall Firmware*\n![](./IMAGES/wavesharematrix/waveshareflash_3.png)\n\n*Yes, go for the firmware and flash it*\n![](./IMAGES/wavesharematrix/waveshareflash_4.png)\n\n*Wait for it..*\n![](./IMAGES/wavesharematrix/waveshareflash_6.png)\n\n*Done! Reflash/Close the page*\n![](./IMAGES/wavesharematrix/waveshareflash_8.png)\n\n#### Test the LED Matrix\n\n*Go to the testing website, connect to the device and test it by hitting the buttons for on/off, left/right, etc.*\n\n![](./IMAGES/wavesharematrix/wavesharetest.png)\n\n\n#### Acquire images with varying\n\nAcquire images using your openUC2 cellphone microscope.\n\n\n## Chapter 3: Digital Microscopy with the Seeed Studio Microscope\n\n### Goals\n\n* Use the Seeed Studio UC2 microscope as a USB webcam\n* Prepare for digital contrast enhancement\n\n### Instructions\n\n1. Assemble the Seeed Studio UC2 microscope with a transmissive LED base and flash the right firmware (**WEBCAM**)\n2. Focus on your sample using the live preview on your computer (macOS: Photo Booth / Windows: Camera App).\n\n\n\n#### 1 Setup the Microscope and flash the right firmware\n\nIn order to get the microscope running it\'s best to use it in a wired mode. We have prepared a firmware that can convert the Wifi-enabled XIAO ESP32 camera into a UVC type webcam so that you can use it from any webcam software using a usb cable.\n\n:::error\nThis process cannot easily be undone! You have to disassemble the microscope to bring the microcontroller into boot mode, so better think about this step twice!\nMore information here: https://openuc2.github.io/docs/Toolboxes/DiscoveryElectronics/04_3_seeedmicroscoperepair#flashing-the-esp32s3-in-case-the-bootloader-is-not-responding\n:::\n\nIn order to flash the firmware, follow the following steps:\n\n##### 1. Reset the firmware\n*This will remove the current firmware. Go to https://espressif.github.io/esptool-js/*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_0.png)\n\n*Connect to the openUC2 Seeed Studio Microscope*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_0_1.png)\n\n*Hit the **Erase Flash** button*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_0_2.png)\n\n*Wait for a moment and then disconnect/close the page*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_0_3.png)\n\n##### 2 Install the new firmware\n\n*Go to our website (https://youseetoo.github.io/) and select the **XIAO WEBCAM FIRMWARE** by clicking on the button*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_1.png)\n\n*Connect to the Seeed Microscope*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_2.png)\n\n*Select the right port*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_5.png)\n\n*Flash the firmware and wait patiently*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_10.png)\n\n*If it says 100% -> well done, refresh/close the page*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_11.png)\n\n##### 3 Use the Webcam\n\n*Open your beloved Webcam Programm and open the camera (e.g. on Mac => Photo Booth, on Windows Camera APP*)\n\n![](./IMAGES/flashxiaowebcam/xiaowebcam_12.png)\n\n*With no light you should only see noise*\n![](./IMAGES/flashxiaowebcam/xiaowebcam_13.png)\n\n\n## Chapter 4: Fluorescence Microscopy Extension\n\n### Goals\n\n* Add fluorescence capability using UV LED and emission filter cubes\n\n#### Instructions\n\n1. Prepare the Microscope and remove the sample holder for now\n2. Prepare the sample holder with the Filter holder and sandwich it between sample and cube.\n3. Test both **transmission mode** and **darkfield mode**.\n4. Handle UV LEDs with care\u2014avoid eye and skin exposure.\n*Get the parts ready. For this you need the fluorescence filter slider and the UV LED/Torch*\n![](./IMAGES/xiaofluoextension/xiaofluoextension_1.jpg)\n\n*Insert an appropriate emission filter (e.g., 500\u2013550 nm bandpass). Sandwich it through the sample insert in the objective lens*\n![](./IMAGES/xiaofluoextension/xiaofluoextension_2.jpg)\n\n*Attach the UV LED cube to the Seeed microscope from above or....*\n![](./IMAGES/xiaofluoextension/xiaofluoextension_3.jpg)\n\n*From the side to enable some sort of darkfield Fluorescence. This has the potential to block more direct light*\n![](./IMAGES/xiaofluoextension/xiaofluoextension_4.jpg)\n\n*Some labelled paper in the focus*\n![](./IMAGES/xiaofluoextension/xiaofluoextension_5.jpg)\n\n*from the side*\n![](./IMAGES/xiaofluoextension/xiaofluoextension_6.jpg)\n\n\n\n#### Result\n\nThis is a z-stack of lens tissue labelled with a yellow text marker (Stabilo) in darkfield configuration. You nicely see the fluorescence that is attached to the Fibers:\n\n![](./IMAGES/flashxiaowebcam/fluoxiao_lenstissue.gif)\n\n## Chapter 5: Digital Phase Contrast using LED Matrix + Webcam\n\n### Goals\n\n* Use programmable lighting for digital differential phase contrast (DPC)\n* Visualize Contrast enhancement in the browser\n* Automate image acquisition and processing via Jupyter Notebook\n\n### Instructions (Standalone via youseetoo.github.io)\n\n1. Replace the UV/normal LED Flashlight with the LED Matrix Cube and go over to https://youseetoo.github.io/indexWebSerialTest.html, connect to the board as described previously and then try different illumination modes -> Capture photos\n\n*Add the led matrix (for Darkfield it actually can also go in the same cube as the sample so that the NA_illu > NA_detection)*\n![](./IMAGES/xiaodpcextension/xiaodpc_1.jpg)\n\n\n*Change Illu settings over the webpage*\n![](./IMAGES/xiaodpcextension/xiaodpc_2.jpg)\n\nYou can turn on the live stream of the camera inside the webpage as described in the following video (Sometimes the camera has to be "reactiveated" - simply select another camera, like the inbuilt one from your laptop or so):\n\n![](./IMAGES/xiaodpcextension/overview.gif)\n\n\n### Instructions (Using Jupyter Notebook)\n\n1. Open the Jupyter Notebook provided here: https://github.com/openUC2/openUC2-SEEED-XIAO-Camera/blob/seeed/JUPYTER/2025_05_01_UC2_Example_DPC.ipynb (click `RAW` to donwnload the `.ipynb`-file)\n2. Run the cell that synchronizes camera snapshots with lighting directions.\n3. Capture four images (top, bottom, left, right illumination).\n4. Compute: `(left - right) / (left + right)` and `(top - bottom) / (top + bottom)`.\n5. Combine into a qualitative phase gradient map resembling DIC microscopy.\n6. Combine into a quantitative phase map using qDPC\n\n\n### DPC and qDPC Imaging with the openUC2 Seeed Studio Microscope\n\n#### Goals\n\nIn this notebook, you will:\n\n- Connect the openUC2 Seeed Studio Microscope as a USB webcam\n- Control the LED matrix to vary illumination patterns (left, right, top, bottom, ring)\n- Acquire images under different directional illuminations\n- Calculate a Differential Phase Contrast (DPC) image\n- Optionally reconstruct a Quantitative DPC (qDPC) phase map\n\n#### Background\n\nBiological samples, especially unstained cells, often exhibit low contrast under brightfield illumination. By controlling the angle of illumination, we can highlight phase gradients\u2014changes in optical density\u2014which are otherwise invisible.\n\nDifferential Phase Contrast (DPC) uses asymmetric illumination to estimate local phase gradients. This enhances image contrast and reveals subcellular structures. The basic mathematicall formular is the following: `(I_top-I_bottom)/(I_top+I_bottom)`. This subtracts the brigthfield image from the gradient image.\n\nQuantitative DPC (qDPC) goes one step further by reconstructing the actual phase distribution of the sample\u2014providing information about refractive index variations or thickness. This is done by computing the (weak) object transfer function and then further deconvolves the images with the resutling response function\n\n*Further reading:*\n- Introduction to Digital Microscopy: https://openuc2.github.io/docs/Toolboxes/DiscoveryElectronics/DigitalMicroscopy\n- Theory and Implementation of DPC: https://openuc2.github.io/docs/Toolboxes/DiscoveryPhaseMicroscopy/DPCmicroscopy/\n\n#### Prerequisites\n\nInstall the following Python libraries before running the notebook:\n\n```bash\npip install https://github.com/openUC2/UC2-REST/archive/refs/heads/master.zip\npip install imageio[ffmpeg]\npip install numpy scipy matplotlib\n\n\n### Getting to know the LED Array\n\n\n```python\n###%%\nimport uc2rest\nimport numpy as np\nimport time\n\n### Connect the LED array to the computer and try to find it\nport = "unknown"\nESP32 = uc2rest.UC2Client(serialport=port, baudrate=115200, DEBUG=False)\n\n### Create LedMatrix object, pass a reference to your \u201cparent\u201d that has post_json()\nmy_led_matrix = ESP32.led\n\n\n### Flash it a few times\nfor i in range(2):\n    # Turn off all LEDs\n    my_led_matrix.send_LEDMatrix_off()\n    time.sleep(0.1)\n    # Fill entire matrix with red\n    my_led_matrix.send_LEDMatrix_full((255,0,0), getReturn=False)\n    time.sleep(0.1)\n\n### Light only left half in bright white\nmDirections = ["left", "right", "top", "bottom"]\nfor iDirection in mDirections:\n    my_led_matrix.send_LEDMatrix_halves(region=iDirection, intensity=(255,255,255), getReturn=False)\n    time.sleep(0.1)\n    mFrames.append(cam.read()[-1])\n\n### Draw a ring of radius 3 in purple\nmy_led_matrix.send_LEDMatrix_rings(radius=3, intensity=(128,0,128))\n\n### Draw a filled circle of radius 5 in green\nmy_led_matrix.send_LEDMatrix_circles(radius=3, intensity=(0,255,0))\n\n### turn on indidivual LEDs\nfor iLED in range(5):\n    # timeout = 0 means no timeout => mResult will be rubish!\n    mResult = ESP32.led.send_LEDMatrix_single(indexled=iLED, intensity=(255, 255, 255), getReturn=0, timeout=0.1)\n    mResult = ESP32.led.send_LEDMatrix_single(indexled=iLED, intensity=(0, 0, 0),  getReturn=0, timeout=0.1)\n\n### display random pattern\nfor i in range(5):\n    led_pattern = np.random.randint(0,55, (25,3))\n    mResult = ESP32.led.send_LEDMatrix_array(led_pattern=led_pattern,getReturn=0,timeout=0)\n\n### turn off\nESP32.led.send_LEDMatrix_full(intensity=(0, 0, 0), getReturn=False)\n\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'[OpenDevice]: Port not found\nUsing API version 2\n{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}\n[SendingCommands]:{"task": "/ledarr_act", "qid": 1, "led": {"action": "off"}}\n')),(0,a.kt)("h3",{id:"getting-to-know-the-camera"},"Getting to know the camera"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import imageio as iio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\n### initialize camera\ncamera = iio.get_reader("<video0>")\n\n### acquire some frames\nfor i in range(10):\n    mFrame = camera.get_next_data()\n    time.sleep(0.1)\n    plt.imshow(mFrame), plt.show()\n\n### close camera\ncamera.close()\n\n')),(0,a.kt)("h3",{id:"acquiring-the-images-at-varying-illumination-and-merge-them"},"Acquiring the images at varying illumination and merge them"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import imageio as iio\nimport matplotlib.pyplot as plt\nimport uc2rest\nimport numpy as np\nimport time\n\n### initialize the LED Array\nport = "unknown"\n###ESP32 = uc2rest.UC2Client(serialport=port, baudrate=115200, DEBUG=True)\n### sometimes the automatic detection doesn\'T work with this board, then find out the port number\n### (e.g. COM3, /dev/cu.usbmodem101, /dev/ttyUSB0  or something - actually the one that you used for flashing on youseetoo.github.io\nport = "/dev/cu.usbmodem101"\nESP32 = uc2rest.UC2Client(serialport=port, baudrate=115200, DEBUG=True, skipFirmwareCheck=True)\n### Create LedMatrix object, pass a reference to your \u201cparent\u201d that has post_json()\nmy_led_matrix = ESP32.led\n\n### initialize camera\ncamera = iio.get_reader("<video0>")\n\nmColour = (0,0,255) # let\'S choose blue to "maximise resolution"\n\n### store the images\nmFrames = []\n### Light only left half in bright white\nmDirections = ["left", "right", "top", "bottom"]\nfor iDirection in mDirections:\n    my_led_matrix.send_LEDMatrix_halves(region=iDirection, intensity=mColour, getReturn=False)\n    time.sleep(0.1)\n    # the xiao camera has some buffer that we need to free everytime for frame synchronisation..\n    for i in range(10):\n        mFrame = camera.get_next_data()\n        time.sleep(0.1)\n    mFrames.append(mFrame)\n    plt.imshow(mFrame), plt.show()\ncamera.close()\nmy_led_matrix.send_LEDMatrix_off()\n\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'Using API version 2\n{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}\n[SendingCommands]:{"task": "/ledarr_act", "qid": 1, "led": {"action": "halves", "region": "left", "r": 0, "g": 0, "b": 255}}\n\n[ProcessLines]:+\n[ProcessLines]:{qid1"ucs"1-\n')),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(23041).Z,width:"552",height:"328"})),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'[SendingCommands]:{"task": "/ledarr_act", "qid": 2, "led": {"action": "halves", "region": "right", "r": 0, "g": 0, "b": 255}}\n\n[ProcessLines]:++\nFailed to read the line in serial: device reports readiness to read but returned no data (device disconnected or multiple access on port?)\nFailed to read the line in serial: \'bool\' object has no attribute \'decode\'\nFailed to read the line in serial: device reports readiness to read but returned no data (device disconnected or multiple access on port?)\nFailed to read the line in serial: \'bool\' object has no attribute \'decode\'\nFailed to load the json from serial\nError: Expecting value: line 1 column 1 (char 0)\n')),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(89663).Z,width:"552",height:"328"})),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'[SendingCommands]:{"task": "/ledarr_act", "qid": 3, "led": {"action": "halves", "region": "top", "r": 0, "g": 0, "b": 255}}\n\n[ProcessLines]:+\nFailed to read the line in serial: device reports readiness to read but returned no data (device disconnected or multiple access on port?)\nFailed to read the line in serial: \'bool\' object has no attribute \'decode\'\nReconnecting to the serial device\nReconnected to the serial device\n')),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(79930).Z,width:"552",height:"328"})),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'[SendingCommands]:{"task": "/ledarr_act", "qid": 1, "led": {"action": "halves", "region": "bottom", "r": 0, "g": 0, "b": 255}}\n\nFailed to read the line in serial: device reports readiness to read but returned no data (device disconnected or multiple access on port?)\nFailed to read the line in serial: \'bool\' object has no attribute \'decode\'\n[ProcessLines]:{"qid":1,"success":1}\n[ProcessLines]:--\nFailed to load the json from serial\nError: Expecting value: line 1 column 1 (char 0)\n[ProcessLines]:+{"state":r"deifr_":2.,"en_4 0592:2,ietfe_,"conf":0,"pindef":"waveshare_esp32s3_ledarray","I2C_SLAVE":0},"qid":0}\n[ProcessLines]:--\nFailed to load the json from serial\nError: Expecting value: line 1 column 1 (char 0)\n')),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(96372).Z,width:"552",height:"328"})),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"[SendingCommands]:{\"task\": \"/ledarr_act\", \"qid\": 2, \"led\": {\"action\": \"off\"}}\n\n[ProcessLines]:++\n[ProcessLines]:{\"qid\":2,\"success\":1}\n[ProcessLines]:--\nFailed to load the json from serial\nError: Expecting value: line 1 column 1 (char 0)\nIt takes too long to get a response, we will resend the last command: {'task': '/ledarr_act', 'qid': 2, 'led': {'action': 'off'}}\nFailed to write the line in serial: We have a queue, so after a while we need to resend the wrong command!\nIt takes too long to get a response, we will resend the last command: {'task': '/ledarr_act', 'qid': 2, 'led': {'action': 'off'}}\nFailed to write the line in serial: We have a queue, so after a while we need to resend the wrong command!\nIt takes too long to get a response, we will resend the last command: {'task': '/ledarr_act', 'qid': 2, 'led': {'action': 'off'}}\nFailed to write the line in serial: We have a queue, so after a while we need to resend the wrong command!\nIt takes too long to get a response, we will resend the last command: {'task': '/ledarr_act', 'qid': 2, 'led': {'action': 'off'}}\nFailed to write the line in serial: We have a queue, so after a while we need to resend the wrong command!\n\n\n\n\n\n'No response received'\n\n\n\n[ProcessCommands]: {'qid': 2, 'success': 1}\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"### Visualize the different images\nplt.subplot(221)\nplt.imshow(mFrames[0])\nplt.subplot(222)\nplt.imshow(mFrames[1])\nplt.subplot(223)\nplt.imshow(mFrames[2])\nplt.subplot(224)\nplt.imshow(mFrames[3])\n\n\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x16c76e550>\n")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(31997).Z,width:"552",height:"377"})),(0,a.kt)("h3",{id:"compute-the-dpc-images"},"Compute the DPC images"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"ATTENTION:")," Ensure that the images are TOP/BOTTOM, LEFT/RIGHT illumination, you can adjust the indices mFrames 0..3"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"iChannel = 1 # play with this to minimize/maximize crosstalk - against the rules anyway\n\n### extract the colour channel\ndpcLeft = np.float32(mFrames[2][:,:,iChannel])\ndpcRight = np.float32(mFrames[3][:,:,iChannel])\ndpcTop = np.float32(mFrames[0][:,:,iChannel])\ndpcBottom = np.float32(mFrames[1][:,:,iChannel])\n\ndpcVert = (dpcLeft-dpcRight)/(dpcLeft+dpcRight)\ndpcHorz = (dpcTop-dpcBottom)/(dpcTop+dpcBottom)\n\n### display\n\nplt.subplot(121)\nplt.title(\"DPC Vertical\")\nplt.imshow(dpcVert, cmap='gray')\nplt.subplot(122)\nplt.title(\"DPC Horizontal\")\nplt.imshow(dpcHorz,cmap='gray')\n\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"<matplotlib.image.AxesImage at 0x17e0b8990>\n")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(20073).Z,width:"552",height:"192"})),(0,a.kt)("h3",{id:"introduce-the-qdpc-model"},"Introduce the qDPC Model"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nfrom scipy.ndimage import uniform_filter\npi    = np.pi\nnaxis = np.newaxis\nF     = lambda x: np.fft.fft2(x)\nIF    = lambda x: np.fft.ifft2(x)\n\ndef pupilGen(fxlin, fylin, wavelength, na, na_in=0.0):\n    pupil = np.array(fxlin[naxis, :]**2+fylin[:, naxis]**2 <= (na/wavelength)**2)\n    if na_in != 0.0:\n        pupil[fxlin[naxis, :]**2+fylin[:, naxis]**2 < (na_in/wavelength)**2] = 0.0\n    return pupil\n\ndef _genGrid(size, dx):\n    xlin = np.arange(size, dtype='complex128')\n    return (xlin-size//2)*dx\n\nclass DPCSolver:\n    def __init__(self, dpc_imgs, wavelength, na, na_in, pixel_size, rotation, dpc_num=4):\n        self.wavelength = wavelength\n        self.na         = na\n        self.na_in      = na_in\n        self.pixel_size = pixel_size\n        self.dpc_num    = 4\n        self.rotation   = rotation\n        self.fxlin      = np.fft.ifftshift(_genGrid(dpc_imgs.shape[-1], 1.0/dpc_imgs.shape[-1]/self.pixel_size))\n        self.fylin      = np.fft.ifftshift(_genGrid(dpc_imgs.shape[-2], 1.0/dpc_imgs.shape[-2]/self.pixel_size))\n        self.dpc_imgs   = dpc_imgs.astype('float64')\n        self.normalization()\n        self.pupil      = pupilGen(self.fxlin, self.fylin, self.wavelength, self.na)\n        self.sourceGen()\n        self.WOTFGen()\n\n    def setTikhonovRegularization(self, reg_u = 1e-6, reg_p = 1e-6):\n        self.reg_u      = reg_u\n        self.reg_p      = reg_p\n\n    def normalization(self):\n        for img in self.dpc_imgs:\n            img          /= uniform_filter(img, size=img.shape[0]//2)\n            meanIntensity = img.mean()\n            img          /= meanIntensity        # normalize intensity with DC term\n            img          -= 1.0                  # subtract the DC term\n\n    def sourceGen(self):\n        self.source = []\n        pupil       = pupilGen(self.fxlin, self.fylin, self.wavelength, self.na, na_in=self.na_in)\n        for rotIdx in range(self.dpc_num):\n            self.source.append(np.zeros((self.dpc_imgs.shape[-2:])))\n            rotdegree = self.rotation[rotIdx]\n            if rotdegree < 180:\n                self.source[-1][self.fylin[:, naxis]*np.cos(np.deg2rad(rotdegree))+1e-15>=\n                                self.fxlin[naxis, :]*np.sin(np.deg2rad(rotdegree))] = 1.0\n                self.source[-1] *= pupil\n            else:\n                self.source[-1][self.fylin[:, naxis]*np.cos(np.deg2rad(rotdegree))+1e-15<\n                                self.fxlin[naxis, :]*np.sin(np.deg2rad(rotdegree))] = -1.0\n                self.source[-1] *= pupil\n                self.source[-1] += pupil\n        self.source = np.asarray(self.source)\n\n    def WOTFGen(self):\n        self.Hu = []\n        self.Hp = []\n        for rotIdx in range(self.source.shape[0]):\n            FSP_cFP  = F(self.source[rotIdx]*self.pupil)*F(self.pupil).conj()\n            I0       = (self.source[rotIdx]*self.pupil*self.pupil.conj()).sum()\n            self.Hu.append(2.0*IF(FSP_cFP.real)/I0)\n            self.Hp.append(2.0j*IF(1j*FSP_cFP.imag)/I0)\n        self.Hu = np.asarray(self.Hu)\n        self.Hp = np.asarray(self.Hp)\n\n    def solve(self, xini=None, plot_verbose=False, **kwargs):\n        dpc_result  = []\n        AHA         = [(self.Hu.conj()*self.Hu).sum(axis=0)+self.reg_u,            (self.Hu.conj()*self.Hp).sum(axis=0),\\\n                       (self.Hp.conj()*self.Hu).sum(axis=0)           , (self.Hp.conj()*self.Hp).sum(axis=0)+self.reg_p]\n        determinant = AHA[0]*AHA[3]-AHA[1]*AHA[2]\n        for frame_index in range(self.dpc_imgs.shape[0]//self.dpc_num):\n            fIntensity = np.asarray([F(self.dpc_imgs[frame_index*self.dpc_num+image_index]) for image_index in range(self.dpc_num)])\n            AHy        = np.asarray([(self.Hu.conj()*fIntensity).sum(axis=0), (self.Hp.conj()*fIntensity).sum(axis=0)])\n            absorption = IF((AHA[3]*AHy[0]-AHA[1]*AHy[1])/determinant).real\n            phase      = IF((AHA[0]*AHy[1]-AHA[2]*AHy[0])/determinant).real\n            dpc_result.append(absorption+1.0j*phase)\n\n        return np.asarray(dpc_result)\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np\nimport matplotlib.pyplot as plt\nfrom os import listdir\nfrom skimage import io\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'\ndpc_images = np.array((dpcLeft, dpcRight, dpcTop, dpcBottom))\n\n\n###plot first set of measured DPC measurements\nf, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(6, 6))\n\nfor plot_index in range(4):\n    plot_row = plot_index//2\n    plot_col = np.mod(plot_index, 2)\n    ax[plot_row, plot_col].imshow(dpc_images[plot_index], cmap="gray")\n    ax[plot_row, plot_col].axis("off")\n    ax[plot_row, plot_col].set_title("DPC {:02d}".format(plot_index))\nplt.show()\n\n')),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(51794).Z,width:"484",height:"413"})),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"wavelength     =  0.514 #micron\nmag            =   40.0\nna             =   0.40 #numerical aperture\nna_in          =    0.0\npixel_size_cam =    6.5 #pixel size of camera\ndpc_num        =      4 #number of DPC images captured for each absorption and phase frame\npixel_size     = pixel_size_cam/mag\nrotation       = [0, 180, 90, 270] #degree\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"### Initialize DPC Solver\ndpc_solver_obj = DPCSolver(dpc_images, wavelength, na, na_in, pixel_size, rotation, dpc_num=dpc_num)\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'###plot the sources\nmax_na_x = max(dpc_solver_obj.fxlin.real*dpc_solver_obj.wavelength/dpc_solver_obj.na)\nmin_na_x = min(dpc_solver_obj.fxlin.real*dpc_solver_obj.wavelength/dpc_solver_obj.na)\nmax_na_y = max(dpc_solver_obj.fylin.real*dpc_solver_obj.wavelength/dpc_solver_obj.na)\nmin_na_y = min(dpc_solver_obj.fylin.real*dpc_solver_obj.wavelength/dpc_solver_obj.na)\nf, ax  = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(6, 6))\nfor plot_index, source in enumerate(list(dpc_solver_obj.source)):\n    plot_row = plot_index//2\n    plot_col = np.mod(plot_index, 2)\n    ax[plot_row, plot_col].imshow(np.fft.fftshift(dpc_solver_obj.source[plot_index]),\\\n                                  cmap=\'gray\', clim=(0,1), extent=[min_na_x, max_na_x, min_na_y, max_na_y])\n    ax[plot_row, plot_col].axis("off")\n    ax[plot_row, plot_col].set_title("DPC Source {:02d}".format(plot_index))\n    ax[plot_row, plot_col].set_xlim(-1.2, 1.2)\n    ax[plot_row, plot_col].set_ylim(-1.2, 1.2)\n    ax[plot_row, plot_col].set_aspect(1)\n')),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(17820).Z,width:"483",height:"504"})),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'### Visualize the Weak object transfer function (WOTF)\n\n###plot the transfer functions\nf, ax = plt.subplots(2, 4, sharex=True, sharey=True, figsize = (10, 4))\nfor plot_index in range(ax.size):\n    plot_row = plot_index//4\n    plot_col = np.mod(plot_index, 4)\n    divider  = make_axes_locatable(ax[plot_row, plot_col])\n    cax      = divider.append_axes("right", size="5%", pad=0.05)\n    if plot_row == 0:\n        plot = ax[plot_row, plot_col].imshow(np.fft.fftshift(dpc_solver_obj.Hu[plot_col].real), cmap=\'jet\',\\\n                                             extent=[min_na_x, max_na_x, min_na_y, max_na_y], clim=[-2., 2.])\n        ax[plot_row, plot_col].set_title("Absorption WOTF {:02d}".format(plot_col))\n        plt.colorbar(plot, cax=cax, ticks=[-2., 0, 2.])\n    else:\n        plot = ax[plot_row, plot_col].imshow(np.fft.fftshift(dpc_solver_obj.Hp[plot_col].imag), cmap=\'jet\',\\\n                                             extent=[min_na_x, max_na_x, min_na_y, max_na_y], clim=[-.8, .8])\n        ax[plot_row, plot_col].set_title("Phase WOTF {:02d}".format(plot_col))\n        plt.colorbar(plot, cax=cax, ticks=[-.8, 0, .8])\n    ax[plot_row, plot_col].set_xlim(-2.2, 2.2)\n    ax[plot_row, plot_col].set_ylim(-2.2, 2.2)\n    ax[plot_row, plot_col].axis("off")\n    ax[plot_row, plot_col].set_aspect(1)\n')),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(75383).Z,width:"837",height:"358"})),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"###parameters for Tikhonov regurlarization [absorption, phase] ((need to tune this based on SNR)\ndpc_solver_obj.setTikhonovRegularization(reg_u = 1e-1, reg_p = 5e-3)\ndpc_result = dpc_solver_obj.solve()\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'_, axes  = plt.subplots(1, 2, figsize=(10, 6), sharex=True, sharey=True)\ndivider  = make_axes_locatable(axes[0])\ncax_1    = divider.append_axes("right", size="5%", pad=0.05)\nplot     = axes[0].imshow(dpc_result[0].real, clim=[-0.15, 0.02], cmap="gray", extent=[0, dpc_result[0].shape[-1], 0, dpc_result[0].shape[-2]])\naxes[0].axis("off")\nplt.colorbar(plot, cax=cax_1, ticks=[-0.15, 0.02])\naxes[0].set_title("Absorption")\ndivider  = make_axes_locatable(axes[1])\ncax_2    = divider.append_axes("right", size="5%", pad=0.05)\nplot     = axes[1].imshow(dpc_result[0].imag, clim=[-1.0, 3.0], cmap="gray", extent=[0, dpc_result[0].shape[-1], 0, dpc_result[0].shape[-2]])\naxes[1].axis("off")\nplt.colorbar(plot, cax=cax_2, ticks=[-1.0, 3.0])\naxes[1].set_title("Phase")\n')),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"png",src:t(4898).Z,width:"825",height:"236"})),(0,a.kt)("h2",{id:"chapter-6-light-sheet-microscopy-hands-on-optional"},"Chapter 6: Light-Sheet Microscopy Hands-On (Optional)"),(0,a.kt)("h3",{id:"goals"},"Goals"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Explore 3D imaging using a robotic light-sheet setup"),(0,a.kt)("li",{parentName:"ul"},"Maybe build it on your own"),(0,a.kt)("li",{parentName:"ul"},"Learn how to control it using python")),(0,a.kt)("h3",{id:"resources"},"Resources"),(0,a.kt)("p",null,"Please visit the dedicated light-sheet documentation at:\n",(0,a.kt)("a",{parentName:"p",href:"https://openuc2.github.io/docs/Toolboxes/DiscoveryLightsheet/Light_sheet_Fluoresence_microscope"},"https://openuc2.github.io/docs/Toolboxes/DiscoveryLightsheet/Light_sheet_Fluoresence_microscope")," or ",(0,a.kt)("a",{parentName:"p",href:"https://openuc2.com/light-sheet-3"},"https://openuc2.com/light-sheet-3")),(0,a.kt)("h3",{id:"instructions"},"Instructions"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Use the sample stage to scan your specimen through the light-sheet"),(0,a.kt)("li",{parentName:"ul"},"Record and stack images to create a volume"),(0,a.kt)("li",{parentName:"ul"},"Visualize using 3D rendering tools (e.g. napari, use `viewer.layers","[0]",".scale = (10,1,1))``")),(0,a.kt)("p",null,(0,a.kt)("img",{src:t(76199).Z,width:"960",height:"655"})),(0,a.kt)("h2",{id:"summary--discussion"},"Summary & Discussion"),(0,a.kt)("p",null,"By completing this workshop, you've explored the openUC2 toolkit from analog to digital microscopy, built programmable lighting systems, and gained insight into contrast-enhancing techniques. You now have the tools and knowledge to build your own flexible microscopy setup tailored to your biological application."),(0,a.kt)("p",null,"If you enjoyed this, please share your results under ",(0,a.kt)("inlineCode",{parentName:"p"},"#openUC2AQLM")," and consider contributing improvements or documentation back to the openUC2 community!"))}d.isMDXComponent=!0},51794:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_13_0-14a00c0afad89ee4e126e00111f61f96.png"},17820:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_16_0-b2fc109afb82eb99a131362910178f18.png"},75383:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_17_0-4a15d7528f1100c8ea975eb45da26f9a.png"},4898:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_19_1-059648d9b0bef71e1a3bcf80e02da0f8.png"},23041:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_6_1-56db412886ae44c8bbf255c41280014c.png"},89663:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_6_3-c425fa5a0c2632e5654992e344f0345f.png"},79930:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_6_5-196f5dd30f9f8d6a12ba34ec17239249.png"},96372:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_6_7-f72a6207c99a330cd788ebb7af9ad0b3.png"},31997:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_7_1-ccecaa2aa8f255549a634b1939bb9e36.png"},20073:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/output_9_1-1bfd70c4ed4b6ea3c4e767171d861dbe.png"},76199:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/arabidopsis-2be634f68c739f8a7b3a6cc30d6c4625.gif"}}]);