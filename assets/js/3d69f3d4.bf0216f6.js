"use strict";(self.webpackChunkuc_2_docs=self.webpackChunkuc_2_docs||[]).push([[1358],{3905:(e,n,s)=>{s.d(n,{Zo:()=>m,kt:()=>g});var a=s(67294);function t(e,n,s){return n in e?Object.defineProperty(e,n,{value:s,enumerable:!0,configurable:!0,writable:!0}):e[n]=s,e}function i(e,n){var s=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),s.push.apply(s,a)}return s}function r(e){for(var n=1;n<arguments.length;n++){var s=null!=arguments[n]?arguments[n]:{};n%2?i(Object(s),!0).forEach((function(n){t(e,n,s[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(s)):i(Object(s)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(s,n))}))}return e}function o(e,n){if(null==e)return{};var s,a,t=function(e,n){if(null==e)return{};var s,a,t={},i=Object.keys(e);for(a=0;a<i.length;a++)s=i[a],n.indexOf(s)>=0||(t[s]=e[s]);return t}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)s=i[a],n.indexOf(s)>=0||Object.prototype.propertyIsEnumerable.call(e,s)&&(t[s]=e[s])}return t}var l=a.createContext({}),c=function(e){var n=a.useContext(l),s=n;return e&&(s="function"==typeof e?e(n):r(r({},n),e)),s},m=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var s=e.components,t=e.mdxType,i=e.originalType,l=e.parentName,m=o(e,["components","mdxType","originalType","parentName"]),u=c(s),g=t,f=u["".concat(l,".").concat(g)]||u[g]||p[g]||i;return s?a.createElement(f,r(r({ref:n},m),{},{components:s})):a.createElement(f,r({ref:n},m))}));function g(e,n){var s=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var i=s.length,r=new Array(i);r[0]=u;var o={};for(var l in n)hasOwnProperty.call(n,l)&&(o[l]=n[l]);o.originalType=e,o.mdxType="string"==typeof e?e:t,r[1]=o;for(var c=2;c<i;c++)r[c]=s[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,s)}u.displayName="MDXCreateElement"},23988:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var a=s(87462),t=(s(67294),s(3905));const i={},r="Image Processing with ImSwitch",o={unversionedId:"ImSwitch/Advanced/Tutorials/Image-Processing",id:"ImSwitch/Advanced/Tutorials/Image-Processing",title:"Image Processing with ImSwitch",description:"ImSwitch can be used to connect image processing and hardware control. There are multiple ways to do that. Here, we focus on the controller-way, which is the native way to have access on the hardware directly and process incoming frames using standard Python libraries.",source:"@site/docs/05_ImSwitch/Advanced/04_Tutorials/Image-Processing.md",sourceDirName:"05_ImSwitch/Advanced/04_Tutorials",slug:"/ImSwitch/Advanced/Tutorials/Image-Processing",permalink:"/docs/ImSwitch/Advanced/Tutorials/Image-Processing",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"ImSwitch Tutorials",permalink:"/docs/ImSwitch/Advanced/Tutorials/"},next:{title:"Smart Microscopy Workflows with Jupyter Notebooks",permalink:"/docs/ImSwitch/Advanced/Tutorials/Jupyter-Workflows"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Autofocus Controller Tutorial",id:"autofocus-controller-tutorial",level:2},{value:"Basic Controller Structure",id:"basic-controller-structure",level:3},{value:"Hardware Access Examples",id:"hardware-access-examples",level:3},{value:"Camera Control",id:"camera-control",level:4},{value:"Stage Control",id:"stage-control",level:4},{value:"Laser/LED Control",id:"laserled-control",level:4},{value:"Autofocus Algorithm Implementation",id:"autofocus-algorithm-implementation",level:3},{value:"Real-time Processing Example",id:"real-time-processing-example",level:3},{value:"Integration with ImSwitch API",id:"integration-with-imswitch-api",level:3},{value:"General Hardware Access Patterns",id:"general-hardware-access-patterns",level:2},{value:"Detector (Camera) Access",id:"detector-camera-access",level:3},{value:"Positioner (Stage) Access",id:"positioner-stage-access",level:3},{value:"Laser/LED Access",id:"laserled-access",level:3},{value:"Advanced Filtering",id:"advanced-filtering",level:3},{value:"Napari Integration",id:"napari-integration",level:2},{value:"Setting Up Napari with ImSwitch",id:"setting-up-napari-with-imswitch",level:3},{value:"Custom Napari Widgets",id:"custom-napari-widgets",level:3},{value:"Multi-Channel Processing",id:"multi-channel-processing",level:2},{value:"Channel-Specific Processing",id:"channel-specific-processing",level:3},{value:"Batch Processing Workflows",id:"batch-processing-workflows",level:2},{value:"Automated Image Processing Pipeline",id:"automated-image-processing-pipeline",level:3},{value:"Quality Control and Metrics",id:"quality-control-and-metrics",level:2},{value:"Image Quality Assessment",id:"image-quality-assessment",level:3},{value:"Custom Processing Plugins",id:"custom-processing-plugins",level:2},{value:"Creating ImSwitch Processing Plugins",id:"creating-imswitch-processing-plugins",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Optimizing Processing Speed",id:"optimizing-processing-speed",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Resources",id:"resources",level:2}],m={toc:c};function p(e){let{components:n,...s}=e;return(0,t.kt)("wrapper",(0,a.Z)({},m,s,{components:n,mdxType:"MDXLayout"}),(0,t.kt)("h1",{id:"image-processing-with-imswitch"},"Image Processing with ImSwitch"),(0,t.kt)("p",null,"ImSwitch can be used to connect image processing and hardware control. There are multiple ways to do that. Here, we focus on the controller-way, which is the native way to have access on the hardware directly and process incoming frames using standard Python libraries. "),(0,t.kt)("p",null,"For this we take an exemplary Controller, the Autofocus Controller, that takes frames and processes them to compute the sharpest plane :"),(0,t.kt)("h1",{id:"image-processing-with-imswitch-1"},"Image Processing with ImSwitch"),(0,t.kt)("p",null,"ImSwitch provides powerful integration between image processing and hardware control. This tutorial shows you how to create custom controllers that process live camera feeds and control hardware in real-time."),(0,t.kt)("h2",{id:"overview"},"Overview"),(0,t.kt)("p",null,"ImSwitch offers multiple ways to integrate image processing:"),(0,t.kt)("ol",null,(0,t.kt)("li",{parentName:"ol"},(0,t.kt)("strong",{parentName:"li"},"Controller-based Processing"),": Native ImSwitch controllers with direct hardware access"),(0,t.kt)("li",{parentName:"ol"},(0,t.kt)("strong",{parentName:"li"},"Plugin-based Processing"),": Modular processing plugins  "),(0,t.kt)("li",{parentName:"ol"},(0,t.kt)("strong",{parentName:"li"},"External Integration"),": Using ImSwitch API from external processing tools"),(0,t.kt)("li",{parentName:"ol"},(0,t.kt)("strong",{parentName:"li"},"Napari Integration"),": Advanced image analysis workflows")),(0,t.kt)("h2",{id:"autofocus-controller-tutorial"},"Autofocus Controller Tutorial"),(0,t.kt)("p",null,"Based on the ",(0,t.kt)("a",{parentName:"p",href:"https://github.com/openUC2/ImSwitch/blob/master/imswitch/imcontrol/controller/controllers/AutofocusController.py"},"AutofocusController"),", here's how to create image processing controllers:"),(0,t.kt)("h3",{id:"basic-controller-structure"},"Basic Controller Structure"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'from imswitch import IS_HEADLESS\nimport time\nimport numpy as np\nimport threading\nfrom imswitch.imcommon.model import initLogger, APIExport\nfrom ..basecontrollers import ImConWidgetController\nfrom skimage.filters import gaussian\nfrom imswitch.imcommon.framework import Signal\nimport cv2\n\nclass AutofocusController(ImConWidgetController):\n    """Custom controller for autofocus functionality."""\n    \n    # Signals for updating GUI\n    sigUpdateFocusPlot = Signal(object, object)\n    sigUpdateFocusValue = Signal(object)\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__logger = initLogger(self)\n        self.isAutofocusRunning = False\n        \n        # Get hardware managers\n        self.setupHardware()\n        \n    def setupHardware(self):\n        """Initialize hardware connections"""\n        # Get camera (detector)\n        if self._setupInfo.autofocus is not None:\n            self.cameraName = self._setupInfo.autofocus.camera\n            self.stageName = self._setupInfo.autofocus.positioner\n        else:\n            # Use first available devices\n            self.cameraName = self._master.detectorsManager.getAllDeviceNames()[0]\n            self.stageName = self._master.positionersManager.getAllDeviceNames()[0]\n        \n        # Get hardware managers\n        self.camera = self._master.detectorsManager[self.cameraName]\n        self.stage = self._master.positionersManager[self.stageName]\n        \n        # Get laser manager (optional)\n        if self._master.lasersManager.getAllDeviceNames():\n            self.laserName = self._master.lasersManager.getAllDeviceNames()[0]\n            self.laser = self._master.lasersManager[self.laserName]\n        else:\n            self.laser = None\n')),(0,t.kt)("h3",{id:"hardware-access-examples"},"Hardware Access Examples"),(0,t.kt)("h4",{id:"camera-control"},"Camera Control"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'def captureImage(self):\n    """Capture single image from camera"""\n    # Start acquisition if not running\n    if not self.camera.acquisition:\n        self.camera.startAcquisition()\n    \n    # Capture frame\n    frame = self.camera.getLatestFrame()\n    \n    # Convert to numpy array if needed\n    if hasattr(frame, \'getData\'):\n        image = np.array(frame.getData())\n    else:\n        image = np.array(frame)\n    \n    return image\n\ndef setCameraParameters(self, exposure_time=100, gain=1.0):\n    """Set camera parameters"""\n    if hasattr(self.camera, \'setParameter\'):\n        self.camera.setParameter(\'ExposureTime\', exposure_time)\n        self.camera.setParameter(\'Gain\', gain)\n')),(0,t.kt)("h4",{id:"stage-control"},"Stage Control"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'def moveStage(self, axis, position, relative=False):\n    """Move stage to position"""\n    if relative:\n        current_pos = self.stage.position[axis]\n        target_pos = current_pos + position\n    else:\n        target_pos = position\n    \n    # Move stage\n    self.stage.move(axis, target_pos, absolute=not relative)\n    \n    # Wait for movement to complete\n    time.sleep(0.1)\n    while self.stage.isMoving(axis):\n        time.sleep(0.01)\n\ndef getStagePosition(self, axis):\n    """Get current stage position"""\n    return self.stage.position[axis]\n')),(0,t.kt)("h4",{id:"laserled-control"},"Laser/LED Control"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'def setIllumination(self, enabled, power=50):\n    """Control illumination"""\n    if self.laser is not None:\n        if enabled:\n            self.laser.setValue(power)  # Set power (0-100%)\n        else:\n            self.laser.setValue(0)      # Turn off\n    \n    # Or control via LED manager\n    if hasattr(self._master, \'ledsManager\'):\n        led_manager = self._master.ledsManager\n        if led_manager.getAllDeviceNames():\n            led = led_manager[led_manager.getAllDeviceNames()[0]]\n            led.setValue(power if enabled else 0)\n')),(0,t.kt)("h3",{id:"autofocus-algorithm-implementation"},"Autofocus Algorithm Implementation"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'def calculateFocusMetric(self, image):\n    """Calculate focus metric from image"""\n    # Convert to grayscale if needed\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    \n    # Apply Gaussian blur to reduce noise\n    blurred = gaussian(gray, sigma=1)\n    \n    # Calculate focus metrics\n    # Method 1: Variance of Laplacian\n    laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n    focus_score = laplacian.var()\n    \n    # Method 2: Sobel gradient magnitude\n    # sobelx = cv2.Sobel(blurred, cv2.CV_64F, 1, 0, ksize=3)\n    # sobely = cv2.Sobel(blurred, cv2.CV_64F, 0, 1, ksize=3)\n    # focus_score = np.mean(sobelx**2 + sobely**2)\n    \n    return focus_score\n\ndef runAutofocus(self, z_start, z_end, z_step=10, axis=\'Z\'):\n    """Run autofocus scan"""\n    if self.isAutofocusRunning:\n        return\n    \n    self.isAutofocusRunning = True\n    \n    try:\n        # Generate Z positions\n        z_positions = np.arange(z_start, z_end + z_step, z_step)\n        focus_scores = []\n        \n        # Turn on illumination\n        self.setIllumination(True, power=50)\n        time.sleep(0.1)  # Wait for illumination to stabilize\n        \n        for z_pos in z_positions:\n            # Move to Z position\n            self.moveStage(axis, z_pos, relative=False)\n            \n            # Wait for settling\n            time.sleep(0.1)\n            \n            # Capture image\n            image = self.captureImage()\n            \n            # Calculate focus score\n            score = self.calculateFocusMetric(image)\n            focus_scores.append(score)\n            \n            # Update GUI (if available)\n            self.sigUpdateFocusValue.emit(score)\n            \n            # Log progress\n            self.__logger.info(f"Z={z_pos:.1f}, Focus Score={score:.2f}")\n        \n        # Find best focus position\n        best_idx = np.argmax(focus_scores)\n        best_z = z_positions[best_idx]\n        best_score = focus_scores[best_idx]\n        \n        # Move to best position\n        self.moveStage(axis, best_z, relative=False)\n        \n        # Turn off illumination\n        self.setIllumination(False)\n        \n        # Update plot\n        self.sigUpdateFocusPlot.emit(z_positions, focus_scores)\n        \n        self.__logger.info(f"Autofocus complete. Best Z: {best_z:.1f}, Score: {best_score:.2f}")\n        \n        return best_z, best_score\n        \n    finally:\n        self.isAutofocusRunning = False\n')),(0,t.kt)("h3",{id:"real-time-processing-example"},"Real-time Processing Example"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'def startLiveProcessing(self):\n    """Start live image processing"""\n    def processing_loop():\n        while self.isProcessingActive:\n            try:\n                # Capture frame\n                image = self.captureImage()\n                \n                # Process image\n                processed = self.processImage(image)\n                \n                # Make control decisions\n                self.makeControlDecisions(processed)\n                \n                # Small delay to prevent overwhelming the system\n                time.sleep(0.01)\n                \n            except Exception as e:\n                self.__logger.error(f"Processing error: {e}")\n    \n    self.isProcessingActive = True\n    self.processing_thread = threading.Thread(target=processing_loop)\n    self.processing_thread.daemon = True\n    self.processing_thread.start()\n\ndef processImage(self, image):\n    """Custom image processing pipeline"""\n    # Example: Edge detection and analysis\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    edges = cv2.Canny(gray, 50, 150)\n    \n    # Find contours\n    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    return {\n        \'original\': image,\n        \'edges\': edges,\n        \'contours\': contours,\n        \'num_objects\': len(contours)\n    }\n\ndef makeControlDecisions(self, processed_data):\n    """Make hardware control decisions based on image analysis"""\n    num_objects = processed_data[\'num_objects\']\n    \n    # Example: Adjust illumination based on object count\n    if num_objects < 5:\n        # Too few objects - increase illumination\n        self.setIllumination(True, power=min(100, self.current_power + 10))\n    elif num_objects > 20:\n        # Too many objects - decrease illumination  \n        self.setIllumination(True, power=max(10, self.current_power - 10))\n')),(0,t.kt)("h3",{id:"integration-with-imswitch-api"},"Integration with ImSwitch API"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},"@APIExport\ndef runAutofocusAPI(self, z_start: float, z_end: float, z_step: float = 10):\n    \"\"\"API endpoint for autofocus\"\"\"\n    try:\n        best_z, score = self.runAutofocus(z_start, z_end, z_step)\n        return {\n            'success': True,\n            'best_z_position': best_z,\n            'focus_score': score\n        }\n    except Exception as e:\n        return {\n            'success': False,\n            'error': str(e)\n        }\n\n@APIExport  \ndef getFocusScore(self):\n    \"\"\"API endpoint to get current focus score\"\"\"\n    try:\n        image = self.captureImage()\n        score = self.calculateFocusMetric(image)\n        return {\n            'success': True,\n            'focus_score': score\n        }\n    except Exception as e:\n        return {\n            'success': False,\n            'error': str(e)\n        }\n")),(0,t.kt)("h2",{id:"general-hardware-access-patterns"},"General Hardware Access Patterns"),(0,t.kt)("h3",{id:"detector-camera-access"},"Detector (Camera) Access"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},"# Get all available cameras\ncamera_names = self._master.detectorsManager.getAllDeviceNames()\n\n# Access specific camera\ncamera = self._master.detectorsManager[camera_name]\n\n# Start/stop acquisition\ncamera.startAcquisition()\ncamera.stopAcquisition()\n\n# Get latest frame\nframe = camera.getLatestFrame()\n")),(0,t.kt)("h3",{id:"positioner-stage-access"},"Positioner (Stage) Access"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},"# Get all available stages\nstage_names = self._master.positionersManager.getAllDeviceNames()\n\n# Access specific stage\nstage = self._master.positionersManager[stage_name]\n\n# Move stage\nstage.move(axis='X', dist=100, absolute=True)\n\n# Get position\nposition = stage.position['X']\n")),(0,t.kt)("h3",{id:"laserled-access"},"Laser/LED Access"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},"# Get all available lasers\nlaser_names = self._master.lasersManager.getAllDeviceNames()\n\n# Access specific laser\nlaser = self._master.lasersManager[laser_name]\n\n# Set power (0-100%)\nlaser.setValue(50)\n\n# Enable/disable\nlaser.setEnabled(True)\n")),(0,t.kt)("p",null,"This tutorial provides the foundation for creating sophisticated image processing controllers in ImSwitch with full hardware integration."),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'    self._commChannel.sigAutoFocus.connect(self.autoFocus)\n    if not IS_HEADLESS:\n        self._widget.focusButton.clicked.connect(self.focusButton)\n\ndef __del__(self):\n    self._AutofocusThead.quit()\n    self._AutofocusThead.wait()\n    if hasattr(super(), \'__del__\'):\n        super().__del__()\n\ndef focusButton(self):\n    if not self.isAutofusRunning:\n        rangez = float(self._widget.zStepRangeEdit.text())\n        resolutionz = float(self._widget.zStepSizeEdit.text())\n        defocusz = float(self._widget.zBackgroundDefocusEdit.text())\n        self._widget.focusButton.setText(\'Stop\')\n        self.autoFocus(rangez, resolutionz, defocusz)\n    else:\n        self.isAutofusRunning = False\n\n@APIExport(runOnUIThread=True)\ndef autoFocus(self, rangez:int=100, resolutionz:int=10, defocusz:int=0):\n    self.isAutofusRunning = True\n    self._AutofocusThead = threading.Thread(\n        target=self.doAutofocusBackground,\n        args=(rangez, resolutionz, defocusz),\n        daemon=True\n    )\n    self._AutofocusThead.start()\n\n@APIExport(runOnUIThread=True)\ndef stopAutofocus(self):\n    self.isAutofusRunning = False\n\ndef grabCameraFrame(self):\n    return self.camera.getLatestFrame()\n\ndef recordFlatfield(self, nFrames=10, nGauss=16, defocusPosition=200, defocusAxis="Z"):\n    flatfield = []\n    posStart = self.stages.getPosition()[defocusAxis]\n    time.sleep(1)\n    self.stages.move(value=defocusPosition, axis=defocusAxis, is_absolute=False, is_blocking=True)\n    for _ in range(nFrames):\n        flatfield.append(self.grabCameraFrame())\n    flatfield = np.mean(np.array(flatfield), 0)\n    flatfield = gaussian(flatfield, sigma=nGauss)\n    self.stages.move(value=-defocusPosition, axis=defocusAxis, is_absolute=False, is_blocking=True)\n    time.sleep(1)\n    return flatfield\n\ndef doAutofocusBackground(self, rangez=100, resolutionz=10, defocusz=0):\n    self._commChannel.sigAutoFocusRunning.emit(True)\n    mProcessor = FrameProcessor()\n    if defocusz != 0:\n        flatfieldImage = self.recordFlatfield(defocusPosition=defocusz)\n        mProcessor.setFlatfieldFrame(flatfieldImage)\n\n    initialPosition = self.stages.getPosition()["Z"]\n    Nz = int(2 * rangez // resolutionz)\n    relative_positions = np.int32(np.linspace(-abs(rangez), abs(rangez), Nz))\n\n    # Move to the first relative position\n    self.stages.move(value=relative_positions[0], axis="Z", is_absolute=False, is_blocking=True)\n\n    for iz in range(Nz):\n        if not self.isAutofusRunning:\n            break\n        if iz != 0:\n            step = relative_positions[iz] - relative_positions[iz - 1]\n            self.stages.move(value=step, axis="Z", is_absolute=False, is_blocking=True)\n        frame = self.grabCameraFrame()\n        mProcessor.add_frame(frame, iz)\n\n    allfocusvals = np.array(mProcessor.getFocusValueList(Nz))\n    mProcessor.stop()\n\n    if self.isAutofusRunning:\n        coordinates = relative_positions + initialPosition\n        if not IS_HEADLESS:\n            self._widget.focusPlotCurve.setData(coordinates[:len(allfocusvals)], allfocusvals)\n        else:\n            self.sigUpdateFocusPlot.emit(coordinates[:len(allfocusvals)], allfocusvals)\n\n        best_index = np.argmax(allfocusvals)\n        bestzpos_rel = relative_positions[best_index]\n\n        # Move to best focus\n        self.stages.move(value=-2 * rangez, axis="Z", is_absolute=False, is_blocking=True)\n        self.stages.move(value=(rangez + bestzpos_rel), axis="Z", is_absolute=False, is_blocking=True)\n    else:\n        # Return to initial absolute position if stopped\n        self.stages.move(value=initialPosition, axis="Z", is_absolute=True, is_blocking=True)\n\n    self._commChannel.sigAutoFocusRunning.emit(False)\n    self.isAutofusRunning = False\n    if not IS_HEADLESS:\n        self._widget.focusButton.setText(\'Autofocus\')\n\n    final_z = bestzpos_rel + initialPosition if self.isAutofusRunning else initialPosition\n    self.sigUpdateFocusValue.emit({"bestzpos": final_z})\n    return final_z\n')),(0,t.kt)("p",null,"class FrameProcessor:\ndef ",(0,t.kt)("strong",{parentName:"p"},"init"),"(self, nGauss=7, nCropsize=2048):\nself.isRunning = True\nself.frame_queue = queue.Queue()\nself.allfocusvals = []\nself.worker_thread = threading.Thread(target=self.process_frames, daemon=True)\nself.worker_thread.start()\nself.flatFieldFrame = None\nself.nGauss = nGauss\nself.nCropsize = nCropsize"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'def setFlatfieldFrame(self, flatfieldFrame):\n    self.flatFieldFrame = flatfieldFrame\n\ndef add_frame(self, img, iz):\n    self.frame_queue.put((img, iz))\n\ndef process_frames(self):\n    while self.isRunning:\n        img, iz = self.frame_queue.get()\n        self.process_frame(img, iz)\n\ndef process_frame(self, img, iz):\n    if self.flatFieldFrame is not None:\n        img = img / self.flatFieldFrame\n    img = self.extract(img, self.nCropsize)\n    if len(img.shape) > 2:\n        img = np.mean(img, -1)\n    if 0:\n        imagearraygf = ndi.gaussian_filter(img, self.nGauss)\n        is_success, buffer = cv2.imencode(".jpg", imagearraygf, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n        focusquality = len(buffer) if is_success else 0\n    else:\n        focusquality = self.calculate_focus_measure(img)\n    self.allfocusvals.append(focusquality)\n\ndef calculate_focus_measure(self, image, method="LAPE"):\n    if len(image.shape) == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  # optional\n    if method == "LAPE":\n        if image.dtype == np.uint16:\n            lap = cv2.Laplacian(image, cv2.CV_32F)\n        else:\n            lap = cv2.Laplacian(image, cv2.CV_16S)\n        focus_measure = np.mean(np.square(lap))\n    elif method == "GLVA":\n        focus_measure = np.std(image, axis=None)  # GLVA\n    else:\n        focus_measure = np.std(image, axis=None)  # GLVA\n    return focus_measure\n\n\n\n\n@staticmethod\ndef extract(marray, crop_size):\n    center_x, center_y = marray.shape[1] // 2, marray.shape[0] // 2\n    x_start = center_x - crop_size // 2\n    x_end = x_start + crop_size\n    y_start = center_y - crop_size // 2\n    y_end = y_start + crop_size\n    return marray[y_start:y_end, x_start:x_end]\n\ndef getFocusValueList(self, nFrameExpected, timeout=5):\n    t0 = time.time()\n    while len(self.allfocusvals) < nFrameExpected:\n        time.sleep(0.01)\n        if time.time() - t0 > timeout:\n            break\n    return self.allfocusvals\n\ndef stop(self):\n    self.isRunning = False\n')),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre"},'### Basic Image Enhancement\n\n```python\n# In ImSwitch scripting environment\nimport numpy as np\nimport cv2\nfrom imswitch.imcontrol.model import SignalDesigner\n\nclass RealTimeProcessor:\n    def __init__(self):\n        self.enabled = True\n        self.contrast_factor = 1.5\n        self.brightness_offset = 0\n        \n    def process_frame(self, image):\n        """Process incoming camera frames in real-time"""\n        if not self.enabled:\n            return image\n            \n        # Convert to float for processing\n        img_float = image.astype(np.float32)\n        \n        # Contrast and brightness adjustment\n        processed = img_float * self.contrast_factor + self.brightness_offset\n        \n        # Clip values to valid range\n        processed = np.clip(processed, 0, 255)\n        \n        return processed.astype(np.uint8)\n    \n    def enable_processing(self, enabled=True):\n        """Enable/disable real-time processing"""\n        self.enabled = enabled\n\n# Initialize processor\nprocessor = RealTimeProcessor()\n\n# Connect to ImSwitch camera stream\ndef on_new_frame(image):\n    processed_image = processor.process_frame(image)\n    # Display or save processed image\n    return processed_image\n')),(0,t.kt)("h3",{id:"advanced-filtering"},"Advanced Filtering"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'import scipy.ndimage as ndi\nfrom skimage import filters, morphology\n\nclass AdvancedProcessor:\n    def __init__(self):\n        self.gaussian_sigma = 1.0\n        self.threshold_method = \'otsu\'\n        \n    def denoise_image(self, image):\n        """Apply denoising filters"""\n        # Gaussian denoising\n        denoised = filters.gaussian(image, sigma=self.gaussian_sigma)\n        \n        # Non-local means denoising (for severe noise)\n        # denoised = cv2.fastNlMeansDenoising(image.astype(np.uint8))\n        \n        return denoised\n    \n    def enhance_contrast(self, image):\n        """Enhance image contrast using adaptive histogram equalization"""\n        # CLAHE (Contrast Limited Adaptive Histogram Equalization)\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n        if len(image.shape) == 2:\n            enhanced = clahe.apply(image.astype(np.uint8))\n        else:\n            enhanced = np.stack([clahe.apply(image[:,:,i].astype(np.uint8)) \n                               for i in range(image.shape[2])], axis=2)\n        return enhanced\n    \n    def segment_objects(self, image):\n        """Segment objects using threshold and morphological operations"""\n        # Automatic thresholding\n        if self.threshold_method == \'otsu\':\n            threshold = filters.threshold_otsu(image)\n        elif self.threshold_method == \'li\':\n            threshold = filters.threshold_li(image)\n        \n        binary = image > threshold\n        \n        # Morphological operations to clean up segmentation\n        binary = morphology.remove_small_objects(binary, min_size=50)\n        binary = morphology.remove_small_holes(binary, area_threshold=64)\n        \n        return binary, threshold\n\n# Usage example\nprocessor = AdvancedProcessor()\n\ndef process_microscopy_image(image):\n    # Step 1: Denoise\n    denoised = processor.denoise_image(image)\n    \n    # Step 2: Enhance contrast\n    enhanced = processor.enhance_contrast(denoised)\n    \n    # Step 3: Segment objects (if needed)\n    segmented, threshold = processor.segment_objects(enhanced)\n    \n    return {\n        \'original\': image,\n        \'denoised\': denoised,\n        \'enhanced\': enhanced, \n        \'segmented\': segmented,\n        \'threshold\': threshold\n    }\n')),(0,t.kt)("h2",{id:"napari-integration"},"Napari Integration"),(0,t.kt)("h3",{id:"setting-up-napari-with-imswitch"},"Setting Up Napari with ImSwitch"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},"import napari\nfrom imswitch.imcontrol.model import ImSwitchController\n\nclass NapariImageProcessor:\n    def __init__(self):\n        self.viewer = napari.Viewer()\n        self.imswitch = ImSwitchController()\n        \n    def connect_to_imswitch(self):\n        \"\"\"Connect Napari to ImSwitch live stream\"\"\"\n        @self.imswitch.signal.new_image.connect\n        def update_napari(image):\n            # Update Napari viewer with new image\n            if 'live_image' in self.viewer.layers:\n                self.viewer.layers['live_image'].data = image\n            else:\n                self.viewer.add_image(image, name='live_image', \n                                    colormap='gray', scale=(1, 1))\n    \n    def add_processing_layers(self, image_data):\n        \"\"\"Add multiple processing results as layers\"\"\"\n        # Original image\n        self.viewer.add_image(image_data['original'], name='Original', \n                            colormap='gray')\n        \n        # Processed versions\n        self.viewer.add_image(image_data['enhanced'], name='Enhanced', \n                            colormap='viridis', visible=False)\n        \n        # Segmentation overlay\n        self.viewer.add_labels(image_data['segmented'].astype(int), \n                             name='Segmentation', opacity=0.5)\n    \n    def setup_measurement_tools(self):\n        \"\"\"Set up measurement and annotation tools\"\"\"\n        # Add shapes layer for measurements\n        shapes_layer = self.viewer.add_shapes(name='Measurements')\n        \n        # Add points layer for marking features\n        points_layer = self.viewer.add_points(name='Feature Points', \n                                            size=10, face_color='red')\n        \n        return shapes_layer, points_layer\n\n# Initialize Napari processor\nnapari_processor = NapariImageProcessor()\nnapari_processor.connect_to_imswitch()\n")),(0,t.kt)("h3",{id:"custom-napari-widgets"},"Custom Napari Widgets"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},"from napari.utils.notifications import show_info\nfrom magicgui import magic_factory\nimport numpy as np\n\n@magic_factory(\n    sigma={'widget_type': 'FloatSlider', 'min': 0.1, 'max': 5.0, 'step': 0.1},\n    threshold={'widget_type': 'FloatSlider', 'min': 0, 'max': 1.0, 'step': 0.01}\n)\ndef process_image_widget(viewer: napari.Viewer, sigma: float = 1.0, threshold: float = 0.5):\n    \"\"\"Interactive image processing widget\"\"\"\n    if len(viewer.layers) == 0:\n        show_info(\"Please load an image first\")\n        return\n    \n    # Get current image\n    image = viewer.layers[0].data\n    \n    # Apply Gaussian filter\n    filtered = filters.gaussian(image, sigma=sigma)\n    \n    # Apply threshold\n    binary = filtered > (threshold * filtered.max())\n    \n    # Update or add processed layers\n    if 'Filtered' in [layer.name for layer in viewer.layers]:\n        viewer.layers['Filtered'].data = filtered\n    else:\n        viewer.add_image(filtered, name='Filtered', colormap='plasma')\n    \n    if 'Binary' in [layer.name for layer in viewer.layers]:\n        viewer.layers['Binary'].data = binary\n    else:\n        viewer.add_image(binary.astype(float), name='Binary', colormap='red')\n\n# Add widget to Napari\n# viewer.window.add_dock_widget(process_image_widget, area='right')\n")),(0,t.kt)("h2",{id:"multi-channel-processing"},"Multi-Channel Processing"),(0,t.kt)("h3",{id:"channel-specific-processing"},"Channel-Specific Processing"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},"class MultiChannelProcessor:\n    def __init__(self):\n        self.channel_configs = {\n            'DAPI': {'wavelength': 405, 'exposure': 100, 'gain': 1.0},\n            'GFP': {'wavelength': 488, 'exposure': 200, 'gain': 1.5},\n            'RFP': {'wavelength': 561, 'exposure': 150, 'gain': 1.2}\n        }\n        \n    def process_channel(self, image, channel_name):\n        \"\"\"Apply channel-specific processing\"\"\"\n        config = self.channel_configs.get(channel_name, {})\n        \n        # Channel-specific noise reduction\n        if channel_name == 'DAPI':\n            # Strong denoising for nuclear staining\n            processed = cv2.bilateralFilter(image, 9, 75, 75)\n        elif channel_name in ['GFP', 'RFP']:\n            # Gentle denoising for fluorescent proteins\n            processed = filters.gaussian(image, sigma=0.8)\n        else:\n            processed = image\n            \n        return processed\n    \n    def align_channels(self, channels):\n        \"\"\"Align multiple channels to correct for chromatic aberration\"\"\"\n        reference_channel = channels[0]\n        aligned_channels = [reference_channel]\n        \n        for channel in channels[1:]:\n            # Calculate phase correlation for alignment\n            shift, error, diffphase = phase_cross_correlation(\n                reference_channel, channel, upsample_factor=100)\n            \n            # Apply shift correction\n            aligned = ndi.shift(channel, shift)\n            aligned_channels.append(aligned)\n            \n        return aligned_channels\n    \n    def create_composite(self, channels, channel_names, colors=None):\n        \"\"\"Create RGB composite from multiple channels\"\"\"\n        if colors is None:\n            colors = ['blue', 'green', 'red'][:len(channels)]\n        \n        # Normalize each channel\n        normalized_channels = []\n        for channel in channels:\n            norm_channel = (channel - channel.min()) / (channel.max() - channel.min())\n            normalized_channels.append(norm_channel)\n        \n        # Create RGB composite\n        composite = np.zeros((*channels[0].shape, 3))\n        color_map = {'blue': 2, 'green': 1, 'red': 0}\n        \n        for channel, color in zip(normalized_channels, colors):\n            if color in color_map:\n                composite[:, :, color_map[color]] = channel\n                \n        return composite\n\n# Usage example\nmc_processor = MultiChannelProcessor()\n\ndef process_multichannel_image(image_stack, channel_names):\n    \"\"\"Process a multi-channel image stack\"\"\"\n    # Split channels\n    channels = [image_stack[:, :, i] for i in range(image_stack.shape[2])]\n    \n    # Process each channel\n    processed_channels = []\n    for channel, name in zip(channels, channel_names):\n        processed = mc_processor.process_channel(channel, name)\n        processed_channels.append(processed)\n    \n    # Align channels\n    aligned_channels = mc_processor.align_channels(processed_channels)\n    \n    # Create composite\n    composite = mc_processor.create_composite(aligned_channels, channel_names)\n    \n    return {\n        'original_channels': channels,\n        'processed_channels': processed_channels,\n        'aligned_channels': aligned_channels,\n        'composite': composite\n    }\n")),(0,t.kt)("h2",{id:"batch-processing-workflows"},"Batch Processing Workflows"),(0,t.kt)("h3",{id:"automated-image-processing-pipeline"},"Automated Image Processing Pipeline"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'import os\nfrom pathlib import Path\nimport tifffile\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nclass BatchProcessor:\n    def __init__(self, input_dir, output_dir):\n        self.input_dir = Path(input_dir)\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True)\n        \n    def process_single_image(self, image_path):\n        """Process a single image file"""\n        try:\n            # Load image\n            image = tifffile.imread(image_path)\n            \n            # Apply processing pipeline\n            results = self.processing_pipeline(image)\n            \n            # Save results\n            output_path = self.output_dir / f"processed_{image_path.name}"\n            tifffile.imwrite(output_path, results[\'enhanced\'])\n            \n            # Save segmentation if available\n            if \'segmented\' in results:\n                seg_path = self.output_dir / f"segmented_{image_path.name}"\n                tifffile.imwrite(seg_path, results[\'segmented\'].astype(np.uint8) * 255)\n            \n            return f"Processed: {image_path.name}"\n            \n        except Exception as e:\n            return f"Error processing {image_path.name}: {str(e)}"\n    \n    def processing_pipeline(self, image):\n        """Define your processing pipeline here"""\n        # Example pipeline\n        denoised = filters.gaussian(image, sigma=1.0)\n        enhanced = exposure.equalize_adapthist(denoised)\n        \n        # Segmentation\n        threshold = filters.threshold_otsu(enhanced)\n        segmented = enhanced > threshold\n        \n        return {\n            \'enhanced\': enhanced,\n            \'segmented\': segmented\n        }\n    \n    def process_batch(self, file_pattern="*.tif", max_workers=4):\n        """Process all images in input directory"""\n        image_files = list(self.input_dir.glob(file_pattern))\n        \n        if not image_files:\n            print(f"No images found matching pattern {file_pattern}")\n            return\n        \n        print(f"Processing {len(image_files)} images...")\n        \n        # Process images in parallel\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            future_to_file = {executor.submit(self.process_single_image, img_file): img_file \n                             for img_file in image_files}\n            \n            for future in as_completed(future_to_file):\n                result = future.result()\n                print(result)\n        \n        print("Batch processing completed!")\n\n# Usage\nbatch_processor = BatchProcessor(\n    input_dir="/path/to/input/images",\n    output_dir="/path/to/output/images"\n)\n\nbatch_processor.process_batch(file_pattern="*.tif", max_workers=4)\n')),(0,t.kt)("h2",{id:"quality-control-and-metrics"},"Quality Control and Metrics"),(0,t.kt)("h3",{id:"image-quality-assessment"},"Image Quality Assessment"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'class ImageQualityAssessment:\n    def __init__(self):\n        self.metrics = {}\n        \n    def calculate_snr(self, image, signal_region=None, noise_region=None):\n        """Calculate Signal-to-Noise Ratio"""\n        if signal_region is None:\n            # Use center region as signal\n            h, w = image.shape[:2]\n            signal_region = image[h//4:3*h//4, w//4:3*w//4]\n        \n        if noise_region is None:\n            # Use corner regions as noise\n            noise_region = np.concatenate([\n                image[:50, :50].flatten(),\n                image[-50:, -50:].flatten()\n            ])\n        \n        signal_mean = np.mean(signal_region)\n        noise_std = np.std(noise_region)\n        \n        snr = signal_mean / noise_std if noise_std > 0 else float(\'inf\')\n        return snr\n    \n    def calculate_focus_measure(self, image):\n        """Calculate focus quality using Laplacian variance"""\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n        laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n        focus_measure = laplacian.var()\n        return focus_measure\n    \n    def detect_blur(self, image, threshold=100):\n        """Detect if image is blurry"""\n        focus_measure = self.calculate_focus_measure(image)\n        return focus_measure < threshold\n    \n    def calculate_brightness_metrics(self, image):\n        """Calculate brightness statistics"""\n        return {\n            \'mean_intensity\': np.mean(image),\n            \'median_intensity\': np.median(image),\n            \'std_intensity\': np.std(image),\n            \'min_intensity\': np.min(image),\n            \'max_intensity\': np.max(image)\n        }\n    \n    def assess_image_quality(self, image):\n        """Comprehensive image quality assessment"""\n        quality_metrics = {\n            \'snr\': self.calculate_snr(image),\n            \'focus_measure\': self.calculate_focus_measure(image),\n            \'is_blurry\': self.detect_blur(image),\n            \'brightness\': self.calculate_brightness_metrics(image)\n        }\n        \n        return quality_metrics\n\n# Usage in processing pipeline\nqa = ImageQualityAssessment()\n\ndef quality_controlled_processing(image):\n    """Process image only if quality criteria are met"""\n    quality = qa.assess_image_quality(image)\n    \n    # Quality gates\n    if quality[\'snr\'] < 5:\n        print("Warning: Low SNR detected")\n    \n    if quality[\'is_blurry\']:\n        print("Warning: Image appears blurry")\n    \n    if quality[\'brightness\'][\'mean_intensity\'] < 10:\n        print("Warning: Image is very dark")\n    \n    # Proceed with processing if quality is acceptable\n    if quality[\'snr\'] > 3 and not quality[\'is_blurry\']:\n        # Apply processing pipeline\n        processed = process_microscopy_image(image)\n        return processed, quality\n    else:\n        return None, quality\n')),(0,t.kt)("h2",{id:"custom-processing-plugins"},"Custom Processing Plugins"),(0,t.kt)("h3",{id:"creating-imswitch-processing-plugins"},"Creating ImSwitch Processing Plugins"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'from imswitch.imcontrol.model.interfaces import ProcessingInterface\n\nclass CustomProcessingPlugin(ProcessingInterface):\n    def __init__(self):\n        super().__init__()\n        self.name = "Custom Image Processor"\n        self.description = "Custom processing pipeline for specific application"\n        \n    def process(self, image, parameters=None):\n        """Main processing function"""\n        if parameters is None:\n            parameters = self.get_default_parameters()\n        \n        # Implement your custom processing here\n        processed_image = self.custom_algorithm(image, parameters)\n        \n        return processed_image\n    \n    def get_default_parameters(self):\n        """Return default processing parameters"""\n        return {\n            \'noise_reduction\': True,\n            \'enhancement_factor\': 1.5,\n            \'segmentation\': False\n        }\n    \n    def custom_algorithm(self, image, params):\n        """Implement your custom processing algorithm"""\n        result = image.copy()\n        \n        if params[\'noise_reduction\']:\n            result = filters.gaussian(result, sigma=1.0)\n        \n        if params[\'enhancement_factor\'] != 1.0:\n            result = result * params[\'enhancement_factor\']\n        \n        return result\n\n# Register plugin with ImSwitch\n# plugin = CustomProcessingPlugin()\n# imswitch.register_processing_plugin(plugin)\n')),(0,t.kt)("h2",{id:"performance-optimization"},"Performance Optimization"),(0,t.kt)("h3",{id:"optimizing-processing-speed"},"Optimizing Processing Speed"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-python"},'import numba\nfrom numba import jit\n\n@jit(nopython=True)\ndef fast_contrast_enhancement(image, factor):\n    """JIT-compiled contrast enhancement for speed"""\n    output = np.empty_like(image)\n    flat_image = image.flatten()\n    flat_output = output.flatten()\n    \n    for i in range(len(flat_image)):\n        flat_output[i] = min(255, max(0, flat_image[i] * factor))\n    \n    return output\n\nclass OptimizedProcessor:\n    def __init__(self):\n        # Pre-compile JIT functions\n        dummy_image = np.zeros((100, 100), dtype=np.uint8)\n        fast_contrast_enhancement(dummy_image, 1.0)\n        \n    def process_large_image(self, image, tile_size=512):\n        """Process large images in tiles to manage memory"""\n        h, w = image.shape[:2]\n        processed = np.zeros_like(image)\n        \n        for y in range(0, h, tile_size):\n            for x in range(0, w, tile_size):\n                y_end = min(y + tile_size, h)\n                x_end = min(x + tile_size, w)\n                \n                tile = image[y:y_end, x:x_end]\n                processed_tile = self.process_tile(tile)\n                processed[y:y_end, x:x_end] = processed_tile\n        \n        return processed\n    \n    def process_tile(self, tile):\n        """Process individual tile"""\n        return fast_contrast_enhancement(tile, 1.5)\n')),(0,t.kt)("h2",{id:"next-steps"},"Next Steps"),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("strong",{parentName:"li"},(0,t.kt)("a",{parentName:"strong",href:"./Jupyter-Workflows.md"},"Jupyter Workflows"))," - Interactive analysis with Jupyter notebooks"),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("strong",{parentName:"li"},(0,t.kt)("a",{parentName:"strong",href:"./Scripting.md"},"Scripting and Automation"))," - Advanced automation techniques"),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("strong",{parentName:"li"},(0,t.kt)("a",{parentName:"strong",href:"./Multi-Position-Imaging.md"},"Multi-Position Imaging"))," - Automated scanning workflows")),(0,t.kt)("h2",{id:"resources"},"Resources"),(0,t.kt)("ul",null,(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("strong",{parentName:"li"},(0,t.kt)("a",{parentName:"strong",href:"https://napari.org/"},"Napari Documentation"))),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("strong",{parentName:"li"},(0,t.kt)("a",{parentName:"strong",href:"https://scikit-image.org/docs/stable/auto_examples/"},"scikit-image Tutorials"))),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("strong",{parentName:"li"},(0,t.kt)("a",{parentName:"strong",href:"https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html"},"OpenCV Python Tutorials"))),(0,t.kt)("li",{parentName:"ul"},(0,t.kt)("strong",{parentName:"li"},(0,t.kt)("a",{parentName:"strong",href:"https://github.com/openUC2/ImSwitchExamples"},"ImSwitch Processing Examples")))))}p.isMDXComponent=!0}}]);